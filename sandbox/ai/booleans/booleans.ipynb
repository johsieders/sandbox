{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-17T10:44:19.286981Z",
     "start_time": "2025-10-17T10:44:19.283972Z"
    }
   },
   "source": [
    "# KI-Gilde\n",
    "# QAware GmbH, Munich\n",
    "# 11.3.2021\n",
    "# revived 17.10.2025"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### What is this Exercise about?\n",
    "\n",
    "Classifiers analyse datasets whose structure we can only guess but are unable to formalize:\n",
    "We  tell spam easily, but precise rules are hard to come by. Classifiers are trained on a\n",
    "train dataset and tested on a separate test dataset. The resulting assessment of depends on the quality of the date sets.\n",
    "\n",
    "In this exercise, we use data sets with a known structure in order to compare different approaches: perceptrons,\n",
    "neuronal networks with one or two layers and different loss functions.\n",
    "\n",
    "The data sets are given by boolean functions (AND, OR an XOR), or geometrical objects such as\n",
    "half-planes, circles, squares or concentric rings.\n",
    "Each of these functions returns 1 on a subset of $R^2$ (a half-plane, a circle or a square) and 0 on its complement.\n",
    "On concentric rings, the corresponding function alternates between 1 and 0.\n",
    "\n",
    "So, `p = plane([0, 1], 0)` defines the lower half-plane in $R^2$; `p(X)` returns 1 iff $X_1 <= 0$.\n",
    "Likewise, `c = circle(1)` defines a circle with radius 1 around the origin; `c(X)` returns 1\n",
    "if X is in the circle or on its boundary, else 0.\n",
    "\n",
    "AND, OR and half-planes are linearly separable, XOR, circles and squares are not.\n",
    "Let's see how different approaches perform. Concentric rings are particularly difficult."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-10-17T12:50:15.439131Z",
     "start_time": "2025-10-17T12:50:15.434709Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Perceptron as SkPerceptron\n",
    "\n",
    "# uncomment this on jupyterlab\n",
    "# import sys\n",
    "# sys.path.insert(0, '../shared')\n",
    "# from metrics import plot_confusion_matrix #, plot_loss\n",
    "\n",
    "from sandbox.ai.util.metrics import plot_confusion_matrix #, plot_loss\n",
    "from perceptron import Perceptron as MyPerceptron\n",
    "from classifiers import Classifier\n",
    "from classifier_fcts import _and, _xor, _or, plane, circle, square, rings\n",
    "from sandbox.ai.util.utils import flatten_to_long"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Regressor with one layer, mean square error loss\n",
    "This is a regressor turned into a classifier by rounding.\n",
    "Generally not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def regressor21(lr):\n",
    "    M = nn.Sequential(nn.Linear(2, 1), nn.Sigmoid())\n",
    "    optimizer = optim.Adam(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.MSELoss()\n",
    "    # pred = Y rounded to nearest integer with gradient removed\n",
    "    pred_fct = lambda Y: torch.round(Y).detach()\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier with one layer, binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classifier21(lr):\n",
    "    M = nn.Linear(2, 1)\n",
    "    optimizer = optim.Adam(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.BCEWithLogitsLoss()\n",
    "    pred_fct = lambda Y: torch.heaviside(Y, torch.zeros(1))\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier with one layer, cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classifier22(lr):\n",
    "    M = nn.Linear(2, 2)\n",
    "    optimizer = optim.Adam(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    # pred = index of max value on each row (always 0 or 1)\n",
    "    pred_fct = lambda Y: torch.argmax(Y, dim=1).view(-1, 1)\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct, flatten_to_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regressor with two layers, mean square error loss\n",
    "This is a regressor turned into a classifier by rounding.\n",
    "Generally not recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def regressor251(lr):\n",
    "    M = nn.Sequential(nn.Linear(2, 5), nn.Sigmoid(), nn.Linear(5, 1))\n",
    "    optimizer = optim.Adam(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.MSELoss()\n",
    "    # pred = Y rounded to nearest integer with gradient removed\n",
    "    pred_fct = lambda Y: torch.round(Y).detach()\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifiers with two layers, binary cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classifier251(lr):\n",
    "    M = nn.Sequential(nn.Linear(2, 5), nn.Sigmoid(), nn.Linear(5, 1))\n",
    "    optimizer = optim.Adam(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.BCEWithLogitsLoss()\n",
    "    pred_fct = lambda Y: torch.heaviside(Y, torch.zeros(1))\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Classifiers with two layers, cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classifier252(lr):\n",
    "    M = nn.Sequential(nn.Linear(2, 5), nn.Sigmoid(), nn.Linear(5, 2))\n",
    "    optimizer = optim.Adam(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    # pred = index of max value on each row (always 0 or 1)\n",
    "    pred_fct = lambda Y: torch.argmax(Y, dim=1).view(-1, 1)\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct, flatten_to_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Classifiers with three layers, cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def classifier2XX2(lr):\n",
    "    M = nn.Sequential(nn.Linear(2, 8), nn.Linear(8, 3), nn.ReLU(), nn.Linear(3,2))\n",
    "    optimizer = optim.Adagrad(M.parameters(), lr=lr)\n",
    "    loss_fct = nn.CrossEntropyLoss()\n",
    "    # pred = index of max value on each row (always 0 or 1)\n",
    "    pred_fct = lambda Y: torch.argmax(Y, dim=1).view(-1, 1)\n",
    "    return Classifier(M, optimizer, loss_fct, pred_fct, flatten_to_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  A plot utility\n",
    "must go here for neat output (??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss(protocol, title='', label=''):\n",
    "    \"\"\"\n",
    "    @param protocol: list of tuples (timestamp, loss)\n",
    "    @param title: title of graph\n",
    "    @param label: label of graph\n",
    "    @return: None\n",
    "    Plot change of loss over time\n",
    "    \"\"\"\n",
    "    plt.plot(protocol, label=label)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_raw = [[0, 0], [0, 1], [1, 0], [1, 1]]\n",
    "dict = {'AND': _and, 'OR': _or, 'XOR': _xor}\n",
    "# variants = [regressor21, classifier21, classifier22, regressor251, classifier251, classifier252, classifier2XX2]\n",
    "variants = [regressor251, classifier251, classifier252, classifier2XX2]\n",
    "\n",
    "cat_names = ['incorrect', 'correct']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the self-made perceptron on AND, OR and XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for op, f in dict.items():\n",
    "    X = tensor(X_raw, dtype=torch.float)\n",
    "    Y = f(X)\n",
    "    p = MyPerceptron(2, lr=0.3)\n",
    "    p.fit(X, Y, n_epochs=6)\n",
    "    Y_p = p.predict(X)\n",
    "\n",
    "    plot_confusion_matrix(Y, Y_p, op, cat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Sklearn perceptron on AND, OR and XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for op, f in dict.items():\n",
    "    X = tensor(X_raw, dtype=torch.float)\n",
    "    Y = f(X)\n",
    "    p = SkPerceptron()\n",
    "    p.fit(X, Y)\n",
    "    Y_p = p.predict(X)\n",
    "\n",
    "    plot_confusion_matrix(Y, Y_p, op, cat_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing all NN variants on AND, OR and XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lr = 1e-1\n",
    "n_epochs = 100\n",
    "\n",
    "for op, f in dict.items():\n",
    "    print('\\n', op)\n",
    "    plt.figure()\n",
    "    for v in variants:\n",
    "        X = tensor(X_raw, requires_grad=True, dtype=torch.float32)\n",
    "        Y = f(X.detach())\n",
    "        clf = v(lr)\n",
    "        protocol = clf.fit(X, Y, n_epochs)\n",
    "        plot_loss(protocol, op, v.__name__)\n",
    "        Y_p = clf.predict(X)\n",
    "        diff = Y_p - Y.view(-1, 1)\n",
    "        print(f'Errors in {v.__name__ :10}: {len(diff[diff != 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Comparing all NN variants on planes, circles and squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "p0 = plane([-1, 1], 0)    # -x0 + x1 = 0\n",
    "p1 = plane([1, 1], -10)   # x0 + x1 = 10\n",
    "p2 = plane([0, 1], -5)    # x1 = 5\n",
    "c = circle(5)  # radius = 5\n",
    "s = square(5)  # side length = 10\n",
    "r = rings(3)\n",
    "dict1 = {'plane0': p0, 'plane1': p1, 'plane2': p2, 'circle': c, 'square':s, 'rings': r}\n",
    "\n",
    "X = torch.rand((300, 2)) * 10\n",
    "lr = 1e-2\n",
    "n_epochs = 1000\n",
    "\n",
    "for op, f in dict1.items():\n",
    "    print('\\n', op)\n",
    "    plt.figure()\n",
    "    for v in variants:\n",
    "        X.requires_grad_()\n",
    "        Y = f(X.detach())\n",
    "        clf = v(lr)\n",
    "        protocol = clf.fit(X, Y, n_epochs)\n",
    "        plot_loss(protocol, op, v.__name__)\n",
    "        Y_p = clf.predict(X)\n",
    "        diff = Y_p - Y.view(-1, 1)\n",
    "        print(f'Errors (train) in {v.__name__ :10}: {len(diff[diff != 0])}')\n",
    "\n",
    "        X_t = torch.rand((300, 2)) * 10\n",
    "        Y_t = f(X_t)\n",
    "        Y_pt = clf.predict(X_t)\n",
    "        diff = Y_pt - Y_t.view(-1, 1)\n",
    "        print(f'Errors (test) in {v.__name__ :10}: {len(diff[diff != 0])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Sklearn perceptron on various functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for op, f in dict1.items():\n",
    "    X.detach_()\n",
    "    Y = f(X)\n",
    "    p = SkPerceptron()\n",
    "    # p = MyPerceptron(2, lr)\n",
    "    p.fit(X, Y)\n",
    "    Y_p = p.predict(X)\n",
    "    plot_confusion_matrix(Y, Y_p, op, cat_names)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
